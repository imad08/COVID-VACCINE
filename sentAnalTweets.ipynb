{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sentAnalTweets.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3qI_gnguK69P",
        "outputId": "74533a1d-ee20-4cf7-9310-58631ff3e637"
      },
      "source": [
        "!pip list | grep allennlp || pip install allennlp"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "allennlp                      2.2.0         \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Ih3nfDjMYat"
      },
      "source": [
        "from typing import Dict, Iterable, List, Tuple\n",
        "\n",
        "import allennlp\n",
        "from allennlp.common.util import JsonDict\n",
        "from allennlp.data import DataLoader, DatasetReader, Instance, Vocabulary, TextFieldTensors\n",
        "from allennlp.data.data_loaders import SimpleDataLoader\n",
        "from allennlp.data.fields import Field, LabelField, TextField\n",
        "from allennlp.data.token_indexers import TokenIndexer, SingleIdTokenIndexer\n",
        "from allennlp.data.tokenizers import Token, Tokenizer, WhitespaceTokenizer\n",
        "from allennlp.models import Model\n",
        "from allennlp.modules import TextFieldEmbedder, Seq2VecEncoder\n",
        "from allennlp.modules.text_field_embedders import BasicTextFieldEmbedder\n",
        "from allennlp.modules.token_embedders import Embedding\n",
        "from allennlp.modules.seq2vec_encoders import BagOfEmbeddingsEncoder\n",
        "from allennlp.nn import util\n",
        "from allennlp.predictors import Predictor\n",
        "from allennlp.training.trainer import GradientDescentTrainer, Trainer\n",
        "from allennlp.training.optimizers import AdamOptimizer\n",
        "from allennlp.training.metrics import CategoricalAccuracy\n",
        "from allennlp.training.util import evaluate\n",
        "from allennlp.predictors.text_classifier import TextClassifierPredictor\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as func\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2sLkVek3cxF"
      },
      "source": [
        "class ClassificationReader(DatasetReader):\n",
        "  def __init__(\n",
        "      self,\n",
        "      tokenizer: Tokenizer = None,\n",
        "      tokenIndexers: Dict[str, TokenIndexer] = None,\n",
        "      maxTokens: int = None,\n",
        "      **kwargs\n",
        "      ):\n",
        "    super().__init__(**kwargs)\n",
        "    self.tokenizer = tokenizer or WhitespaceTokenizer()\n",
        "    self.tokenIndexers = tokenIndexers or {'tokens': SingleIdTokenIndexer()}\n",
        "    self.maxTokens = maxTokens\n",
        "  \n",
        "  def _read(self, filePath: str) ->  Iterable[Instance]:\n",
        "    df = pd.read_csv(filePath)\n",
        "    for i in range(len(df)):\n",
        "      text = df['Text'][i]\n",
        "      textField = TextField(self.tokenizer.tokenize(text), self.tokenIndexers)\n",
        "      label = str(df['sentiment '][i])\n",
        "      labelField = LabelField(label)\n",
        "      fields = {'text': textField, 'label': labelField}\n",
        "      yield Instance(fields)\n",
        "  \n",
        "  def text_to_instance(self, text: str, label: str = None) -> Instance:\n",
        "    tokens = self.tokenizer.tokenize(text)\n",
        "    text_field = TextField(tokens, self.tokenIndexers)\n",
        "    fields = {'text': text_field}\n",
        "    if label:\n",
        "      fields['label'] = LabelField(label)\n",
        "    return Instance(fields)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80sm3GrG3j2P"
      },
      "source": [
        "class Classifier(Model):\n",
        "  def __init__(self,\n",
        "               vocab: Vocabulary,\n",
        "               embedder: TextFieldEmbedder,\n",
        "               encoder: Seq2VecEncoder):\n",
        "    super().__init__(vocab)\n",
        "    self.embedder = embedder\n",
        "    self.encoder = encoder\n",
        "    numLabels = vocab.get_vocab_size('labels')\n",
        "    print(numLabels)\n",
        "    self.classifier = nn.Linear(encoder.get_output_dim(), numLabels)\n",
        "    self.accuracy = CategoricalAccuracy()\n",
        "\n",
        "  def forward(\n",
        "      self,\n",
        "      text: TextFieldTensors,\n",
        "      label: torch.Tensor = None\n",
        "      ) -> Dict[str, torch.Tensor]:\n",
        "    embeddedText = self.embedder(text)\n",
        "    mask = util.get_text_field_mask(text)\n",
        "    encodedText = self.encoder(embeddedText, mask)\n",
        "    logits = self.classifier(encodedText)\n",
        "    probs = func.softmax(logits)\n",
        "    output = {'probs': probs}\n",
        "    if label is not None:\n",
        "      self.accuracy(logits, label)\n",
        "      output['loss'] = func.cross_entropy(logits, label)\n",
        "    \n",
        "    return output\n",
        "  \n",
        "  def get_metrics(self, reset: bool = False) -> Dict[str, float]:\n",
        "    return {'accuracy': self.accuracy.get_metric(reset)}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m7Ii9KtspnQF"
      },
      "source": [
        "def buildVocab(instances: Iterable[Instance]) -> Vocabulary:\n",
        "  print('Building the vocabulary...')\n",
        "  vocab = Vocabulary.from_instances(instances)\n",
        "  print('Built the vocabulary!')\n",
        "  return vocab"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8PAra7ngq4En"
      },
      "source": [
        "def buildModel(vocab: Vocabulary) -> Model:\n",
        "  print('Building the model...')\n",
        "  vocabSize = vocab.get_vocab_size('tokens')\n",
        "  embedder = BasicTextFieldEmbedder(\n",
        "      {'tokens': Embedding(embedding_dim=10, num_embeddings=vocabSize)}\n",
        "  )\n",
        "  encoder = BagOfEmbeddingsEncoder(embedding_dim=10)\n",
        "  model = Classifier(vocab, embedder, encoder)\n",
        "  print('Built the model!')\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wr5iQ7SCSkjq"
      },
      "source": [
        "def buildDatasetReader() -> DatasetReader:\n",
        "  return ClassificationReader()\n",
        "\n",
        "def readData(reader: DatasetReader) -> Tuple[List[Instance], List[Instance]]:\n",
        "  trainingData = list(reader.read('nlp.csv'))[:100]\n",
        "  validationData = list(reader.read('nlp.csv'))[100:195]\n",
        "  return trainingData, validationData\n",
        "\n",
        "def buildDataLoaders(\n",
        "    trainData: List[Instance],\n",
        "    devData: List[Instance],\n",
        ") -> Tuple[DataLoader, DataLoader]:\n",
        "  trainLoader = SimpleDataLoader(trainData, 8, shuffle=True)\n",
        "  devLoader = SimpleDataLoader(devData, 8, shuffle=False)\n",
        "  return trainLoader, devLoader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "logJokeoSt2T"
      },
      "source": [
        "def buildTrainer(\n",
        "    model: Model,\n",
        "    trainLoader: DataLoader,\n",
        "    devLoader: DataLoader,\n",
        "    numEpochs: int\n",
        ") -> Trainer:\n",
        "  parameters = [(n, p) for n, p in model.named_parameters() if p.requires_grad]\n",
        "  optimizer = AdamOptimizer(parameters)\n",
        "  trainer = GradientDescentTrainer(\n",
        "      model=model,\n",
        "      data_loader=trainLoader,\n",
        "      validation_data_loader=devLoader,\n",
        "      num_epochs=numEpochs,\n",
        "      optimizer=optimizer\n",
        "  )\n",
        "  return trainer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5aKo_51ss4YZ"
      },
      "source": [
        "def runTrainingLoop(numEpochs: int):\n",
        "  datasetReader = buildDatasetReader()\n",
        "  \n",
        "  print('Reading data...')\n",
        "  trainData, devData = readData(datasetReader)\n",
        "  print('Reading done!')\n",
        "\n",
        "  vocab = buildVocab(trainData + devData)\n",
        "  model = buildModel(vocab)\n",
        "\n",
        "  trainLoader, devLoader = buildDataLoaders(trainData, devData)\n",
        "  trainLoader.index_with(vocab)\n",
        "  devLoader.index_with(vocab)\n",
        "\n",
        "  trainer = buildTrainer(model, trainLoader, devLoader, numEpochs)\n",
        "  print('Starting training...')\n",
        "  trainer.train()\n",
        "  print('Finished training...')\n",
        "\n",
        "  return model, datasetReader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TCGhznn6r5Pt",
        "outputId": "05b5822e-6019-4413-bb28-ce87292a8e16"
      },
      "source": [
        "model, datasetReader = runTrainingLoop(numEpochs=100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading data...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "building vocab: 100%|##########| 195/195 [00:00<00:00, 14840.22it/s]\n",
            "You provided a validation dataset but patience was set to None, meaning that early stopping is disabled\n",
            "  0%|          | 0/13 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:23: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "accuracy: 0.3100, batch_loss: 1.0986, loss: 1.1157 ||: 100%|##########| 13/13 [00:00<00:00, 233.41it/s]\n",
            "accuracy: 0.4526, batch_loss: 1.0171, loss: 1.0640 ||: 100%|##########| 12/12 [00:00<00:00, 597.52it/s]\n",
            "accuracy: 0.7200, batch_loss: 0.9478, loss: 0.9887 ||: 100%|##########| 13/13 [00:00<00:00, 359.63it/s]\n",
            "  0%|          | 0/12 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Reading done!\n",
            "Building the vocabulary...\n",
            "Built the vocabulary!\n",
            "Building the model...\n",
            "3\n",
            "Built the model!\n",
            "Starting training...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "accuracy: 0.5474, batch_loss: 0.9866, loss: 1.0424 ||: 100%|##########| 12/12 [00:00<00:00, 523.53it/s]\n",
            "accuracy: 0.8100, batch_loss: 0.7233, loss: 0.8701 ||: 100%|##########| 13/13 [00:00<00:00, 263.96it/s]\n",
            "accuracy: 0.4947, batch_loss: 0.9764, loss: 1.0394 ||: 100%|##########| 12/12 [00:00<00:00, 479.27it/s]\n",
            "accuracy: 0.8700, batch_loss: 0.7777, loss: 0.7711 ||: 100%|##########| 13/13 [00:00<00:00, 305.42it/s]\n",
            "accuracy: 0.5368, batch_loss: 0.9696, loss: 1.0454 ||: 100%|##########| 12/12 [00:00<00:00, 736.09it/s]\n",
            "accuracy: 0.8600, batch_loss: 1.0020, loss: 0.6897 ||: 100%|##########| 13/13 [00:00<00:00, 271.02it/s]\n",
            "accuracy: 0.5158, batch_loss: 0.9752, loss: 1.0594 ||: 100%|##########| 12/12 [00:00<00:00, 743.56it/s]\n",
            "accuracy: 0.8600, batch_loss: 0.5326, loss: 0.5891 ||: 100%|##########| 13/13 [00:00<00:00, 317.57it/s]\n",
            "accuracy: 0.5158, batch_loss: 0.9825, loss: 1.0736 ||: 100%|##########| 12/12 [00:00<00:00, 501.10it/s]\n",
            "accuracy: 0.9000, batch_loss: 0.4003, loss: 0.5149 ||: 100%|##########| 13/13 [00:00<00:00, 319.46it/s]\n",
            "accuracy: 0.5158, batch_loss: 0.9937, loss: 1.0962 ||: 100%|##########| 12/12 [00:00<00:00, 495.02it/s]\n",
            "accuracy: 0.9600, batch_loss: 0.4740, loss: 0.4514 ||: 100%|##########| 13/13 [00:00<00:00, 293.69it/s]\n",
            "accuracy: 0.5263, batch_loss: 1.0114, loss: 1.1227 ||: 100%|##########| 12/12 [00:00<00:00, 948.17it/s]\n",
            "accuracy: 0.9600, batch_loss: 0.3132, loss: 0.3911 ||: 100%|##########| 13/13 [00:00<00:00, 319.02it/s]\n",
            "accuracy: 0.5158, batch_loss: 1.0339, loss: 1.1509 ||: 100%|##########| 12/12 [00:00<00:00, 696.87it/s]\n",
            "accuracy: 0.9700, batch_loss: 0.6862, loss: 0.3592 ||: 100%|##########| 13/13 [00:00<00:00, 292.76it/s]\n",
            "accuracy: 0.5158, batch_loss: 1.0538, loss: 1.1739 ||: 100%|##########| 12/12 [00:00<00:00, 526.72it/s]\n",
            "accuracy: 0.9700, batch_loss: 0.5382, loss: 0.3133 ||: 100%|##########| 13/13 [00:00<00:00, 227.56it/s]\n",
            "accuracy: 0.5158, batch_loss: 1.0719, loss: 1.1953 ||: 100%|##########| 12/12 [00:00<00:00, 914.49it/s]\n",
            "accuracy: 0.9800, batch_loss: 0.2768, loss: 0.2703 ||: 100%|##########| 13/13 [00:00<00:00, 327.58it/s]\n",
            "accuracy: 0.5158, batch_loss: 1.0926, loss: 1.2161 ||: 100%|##########| 12/12 [00:00<00:00, 785.34it/s]\n",
            "accuracy: 0.9800, batch_loss: 0.2903, loss: 0.2429 ||: 100%|##########| 13/13 [00:00<00:00, 217.33it/s]\n",
            "accuracy: 0.5053, batch_loss: 1.1104, loss: 1.2381 ||: 100%|##########| 12/12 [00:00<00:00, 559.37it/s]\n",
            "accuracy: 0.9800, batch_loss: 0.0571, loss: 0.2102 ||: 100%|##########| 13/13 [00:00<00:00, 270.94it/s]\n",
            "accuracy: 0.4947, batch_loss: 1.1231, loss: 1.2568 ||: 100%|##########| 12/12 [00:00<00:00, 856.42it/s]\n",
            "accuracy: 0.9800, batch_loss: 0.0525, loss: 0.1903 ||: 100%|##########| 13/13 [00:00<00:00, 339.17it/s]\n",
            "accuracy: 0.4947, batch_loss: 1.1426, loss: 1.2779 ||: 100%|##########| 12/12 [00:00<00:00, 826.46it/s]\n",
            "accuracy: 0.9900, batch_loss: 0.0522, loss: 0.1730 ||: 100%|##########| 13/13 [00:00<00:00, 268.54it/s]\n",
            "accuracy: 0.4947, batch_loss: 1.1613, loss: 1.2975 ||: 100%|##########| 12/12 [00:00<00:00, 773.16it/s]\n",
            "accuracy: 0.9900, batch_loss: 0.0973, loss: 0.1600 ||: 100%|##########| 13/13 [00:00<00:00, 293.76it/s]\n",
            "accuracy: 0.4947, batch_loss: 1.1779, loss: 1.3181 ||: 100%|##########| 12/12 [00:00<00:00, 744.19it/s]\n",
            "accuracy: 0.9900, batch_loss: 0.1457, loss: 0.1486 ||: 100%|##########| 13/13 [00:00<00:00, 269.79it/s]\n",
            "accuracy: 0.4842, batch_loss: 1.1955, loss: 1.3364 ||: 100%|##########| 12/12 [00:00<00:00, 510.43it/s]\n",
            "accuracy: 0.9900, batch_loss: 0.1126, loss: 0.1360 ||: 100%|##########| 13/13 [00:00<00:00, 242.02it/s]\n",
            "accuracy: 0.4842, batch_loss: 1.2145, loss: 1.3556 ||: 100%|##########| 12/12 [00:00<00:00, 500.59it/s]\n",
            "accuracy: 0.9900, batch_loss: 0.1357, loss: 0.1271 ||: 100%|##########| 13/13 [00:00<00:00, 352.78it/s]\n",
            "accuracy: 0.4842, batch_loss: 1.2284, loss: 1.3718 ||: 100%|##########| 12/12 [00:00<00:00, 468.05it/s]\n",
            "accuracy: 0.9900, batch_loss: 0.0659, loss: 0.1151 ||: 100%|##########| 13/13 [00:00<00:00, 231.35it/s]\n",
            "accuracy: 0.4842, batch_loss: 1.2449, loss: 1.3911 ||: 100%|##########| 12/12 [00:00<00:00, 411.78it/s]\n",
            "accuracy: 0.9900, batch_loss: 0.0179, loss: 0.1057 ||: 100%|##########| 13/13 [00:00<00:00, 326.99it/s]\n",
            "accuracy: 0.4737, batch_loss: 1.2601, loss: 1.4066 ||: 100%|##########| 12/12 [00:00<00:00, 493.86it/s]\n",
            "accuracy: 0.9900, batch_loss: 0.1009, loss: 0.1016 ||: 100%|##########| 13/13 [00:00<00:00, 324.38it/s]\n",
            "accuracy: 0.4632, batch_loss: 1.2753, loss: 1.4217 ||: 100%|##########| 12/12 [00:00<00:00, 721.10it/s]\n",
            "accuracy: 0.9900, batch_loss: 0.0323, loss: 0.0927 ||: 100%|##########| 13/13 [00:00<00:00, 241.35it/s]\n",
            "accuracy: 0.4632, batch_loss: 1.2949, loss: 1.4396 ||: 100%|##########| 12/12 [00:00<00:00, 549.55it/s]\n",
            "accuracy: 0.9900, batch_loss: 0.1249, loss: 0.0908 ||: 100%|##########| 13/13 [00:00<00:00, 259.60it/s]\n",
            "accuracy: 0.4632, batch_loss: 1.3106, loss: 1.4537 ||: 100%|##########| 12/12 [00:00<00:00, 523.26it/s]\n",
            "accuracy: 0.9900, batch_loss: 0.0259, loss: 0.0814 ||: 100%|##########| 13/13 [00:00<00:00, 223.49it/s]\n",
            "accuracy: 0.4632, batch_loss: 1.3239, loss: 1.4677 ||: 100%|##########| 12/12 [00:00<00:00, 646.25it/s]\n",
            "accuracy: 0.9900, batch_loss: 0.0119, loss: 0.0761 ||: 100%|##########| 13/13 [00:00<00:00, 220.71it/s]\n",
            "accuracy: 0.4632, batch_loss: 1.3405, loss: 1.4832 ||: 100%|##########| 12/12 [00:00<00:00, 491.15it/s]\n",
            "accuracy: 0.9900, batch_loss: 0.0282, loss: 0.0726 ||: 100%|##########| 13/13 [00:00<00:00, 240.02it/s]\n",
            "accuracy: 0.4632, batch_loss: 1.3544, loss: 1.4967 ||: 100%|##########| 12/12 [00:00<00:00, 574.07it/s]\n",
            "accuracy: 0.9900, batch_loss: 0.0591, loss: 0.0697 ||: 100%|##########| 13/13 [00:00<00:00, 272.88it/s]\n",
            "accuracy: 0.4737, batch_loss: 1.3669, loss: 1.5081 ||: 100%|##########| 12/12 [00:00<00:00, 488.43it/s]\n",
            "accuracy: 0.9900, batch_loss: 0.0922, loss: 0.0675 ||: 100%|##########| 13/13 [00:00<00:00, 314.53it/s]\n",
            "accuracy: 0.4737, batch_loss: 1.3813, loss: 1.5226 ||: 100%|##########| 12/12 [00:00<00:00, 700.85it/s]\n",
            "accuracy: 0.9900, batch_loss: 0.0248, loss: 0.0615 ||: 100%|##########| 13/13 [00:00<00:00, 298.49it/s]\n",
            "accuracy: 0.4737, batch_loss: 1.3957, loss: 1.5344 ||: 100%|##########| 12/12 [00:00<00:00, 509.90it/s]\n",
            "accuracy: 0.9900, batch_loss: 0.0087, loss: 0.0577 ||: 100%|##########| 13/13 [00:00<00:00, 212.21it/s]\n",
            "accuracy: 0.4737, batch_loss: 1.4095, loss: 1.5458 ||: 100%|##########| 12/12 [00:00<00:00, 697.96it/s]\n",
            "accuracy: 0.9900, batch_loss: 0.0123, loss: 0.0551 ||: 100%|##########| 13/13 [00:00<00:00, 260.27it/s]\n",
            "accuracy: 0.4737, batch_loss: 1.4215, loss: 1.5555 ||: 100%|##########| 12/12 [00:00<00:00, 671.61it/s]\n",
            "accuracy: 0.9900, batch_loss: 0.0530, loss: 0.0540 ||: 100%|##########| 13/13 [00:00<00:00, 233.37it/s]\n",
            "accuracy: 0.4737, batch_loss: 1.4358, loss: 1.5686 ||: 100%|##########| 12/12 [00:00<00:00, 822.65it/s]\n",
            "accuracy: 0.9900, batch_loss: 0.0309, loss: 0.0506 ||: 100%|##########| 13/13 [00:00<00:00, 266.00it/s]\n",
            "accuracy: 0.4737, batch_loss: 1.4478, loss: 1.5791 ||: 100%|##########| 12/12 [00:00<00:00, 543.08it/s]\n",
            "accuracy: 0.9900, batch_loss: 0.2934, loss: 0.0583 ||: 100%|##########| 13/13 [00:00<00:00, 205.11it/s]\n",
            "accuracy: 0.4737, batch_loss: 1.4616, loss: 1.5930 ||: 100%|##########| 12/12 [00:00<00:00, 635.08it/s]\n",
            "accuracy: 0.9900, batch_loss: 0.0208, loss: 0.0456 ||: 100%|##########| 13/13 [00:00<00:00, 282.88it/s]\n",
            "accuracy: 0.4737, batch_loss: 1.4759, loss: 1.6054 ||: 100%|##########| 12/12 [00:00<00:00, 654.89it/s]\n",
            "accuracy: 0.9900, batch_loss: 0.0167, loss: 0.0435 ||: 100%|##########| 13/13 [00:00<00:00, 308.61it/s]\n",
            "accuracy: 0.4737, batch_loss: 1.4892, loss: 1.6170 ||: 100%|##########| 12/12 [00:00<00:00, 561.38it/s]\n",
            "accuracy: 0.9900, batch_loss: 0.1037, loss: 0.0449 ||: 100%|##########| 13/13 [00:00<00:00, 292.69it/s]\n",
            "accuracy: 0.4737, batch_loss: 1.5008, loss: 1.6271 ||: 100%|##########| 12/12 [00:00<00:00, 396.17it/s]\n",
            "accuracy: 0.9900, batch_loss: 0.0042, loss: 0.0393 ||: 100%|##########| 13/13 [00:00<00:00, 226.60it/s]\n",
            "accuracy: 0.4737, batch_loss: 1.5086, loss: 1.6326 ||: 100%|##########| 12/12 [00:00<00:00, 368.97it/s]\n",
            "accuracy: 1.0000, batch_loss: 0.0870, loss: 0.0407 ||: 100%|##########| 13/13 [00:00<00:00, 181.70it/s]\n",
            "accuracy: 0.4737, batch_loss: 1.5207, loss: 1.6431 ||: 100%|##########| 12/12 [00:00<00:00, 717.90it/s]\n",
            "accuracy: 1.0000, batch_loss: 0.0239, loss: 0.0367 ||: 100%|##########| 13/13 [00:00<00:00, 204.57it/s]\n",
            "accuracy: 0.4737, batch_loss: 1.5289, loss: 1.6497 ||: 100%|##########| 12/12 [00:00<00:00, 599.28it/s]\n",
            "accuracy: 1.0000, batch_loss: 0.0044, loss: 0.0345 ||: 100%|##########| 13/13 [00:00<00:00, 268.64it/s]\n",
            "accuracy: 0.4632, batch_loss: 1.5413, loss: 1.6598 ||: 100%|##########| 12/12 [00:00<00:00, 706.15it/s]\n",
            "accuracy: 1.0000, batch_loss: 0.0212, loss: 0.0337 ||: 100%|##########| 13/13 [00:00<00:00, 292.41it/s]\n",
            "accuracy: 0.4632, batch_loss: 1.5548, loss: 1.6715 ||: 100%|##########| 12/12 [00:00<00:00, 453.25it/s]\n",
            "accuracy: 1.0000, batch_loss: 0.0073, loss: 0.0319 ||: 100%|##########| 13/13 [00:00<00:00, 273.87it/s]\n",
            "accuracy: 0.4632, batch_loss: 1.5667, loss: 1.6820 ||: 100%|##########| 12/12 [00:00<00:00, 673.07it/s]\n",
            "accuracy: 1.0000, batch_loss: 0.0969, loss: 0.0341 ||: 100%|##########| 13/13 [00:00<00:00, 261.75it/s]\n",
            "accuracy: 0.4632, batch_loss: 1.5772, loss: 1.6907 ||: 100%|##########| 12/12 [00:00<00:00, 303.32it/s]\n",
            "accuracy: 1.0000, batch_loss: 0.0019, loss: 0.0293 ||: 100%|##########| 13/13 [00:00<00:00, 200.33it/s]\n",
            "accuracy: 0.4737, batch_loss: 1.5884, loss: 1.6994 ||: 100%|##########| 12/12 [00:00<00:00, 220.64it/s]\n",
            "accuracy: 1.0000, batch_loss: 0.0063, loss: 0.0284 ||: 100%|##########| 13/13 [00:00<00:00, 191.60it/s]\n",
            "accuracy: 0.4737, batch_loss: 1.6001, loss: 1.7094 ||: 100%|##########| 12/12 [00:00<00:00, 515.52it/s]\n",
            "accuracy: 1.0000, batch_loss: 0.0718, loss: 0.0298 ||: 100%|##########| 13/13 [00:00<00:00, 292.09it/s]\n",
            "accuracy: 0.4737, batch_loss: 1.6119, loss: 1.7196 ||: 100%|##########| 12/12 [00:00<00:00, 675.85it/s]\n",
            "accuracy: 1.0000, batch_loss: 0.0017, loss: 0.0261 ||: 100%|##########| 13/13 [00:00<00:00, 310.74it/s]\n",
            "accuracy: 0.4737, batch_loss: 1.6213, loss: 1.7260 ||: 100%|##########| 12/12 [00:00<00:00, 967.25it/s]\n",
            "accuracy: 1.0000, batch_loss: 0.0028, loss: 0.0252 ||: 100%|##########| 13/13 [00:00<00:00, 265.40it/s]\n",
            "accuracy: 0.4737, batch_loss: 1.6305, loss: 1.7332 ||: 100%|##########| 12/12 [00:00<00:00, 773.29it/s]\n",
            "accuracy: 1.0000, batch_loss: 0.0498, loss: 0.0261 ||: 100%|##########| 13/13 [00:00<00:00, 267.78it/s]\n",
            "accuracy: 0.4737, batch_loss: 1.6423, loss: 1.7438 ||: 100%|##########| 12/12 [00:00<00:00, 545.58it/s]\n",
            "accuracy: 1.0000, batch_loss: 0.0047, loss: 0.0235 ||: 100%|##########| 13/13 [00:00<00:00, 258.89it/s]\n",
            "accuracy: 0.4737, batch_loss: 1.6530, loss: 1.7523 ||: 100%|##########| 12/12 [00:00<00:00, 785.18it/s]\n",
            "accuracy: 1.0000, batch_loss: 0.0031, loss: 0.0227 ||: 100%|##########| 13/13 [00:00<00:00, 234.29it/s]\n",
            "accuracy: 0.4737, batch_loss: 1.6642, loss: 1.7618 ||: 100%|##########| 12/12 [00:00<00:00, 743.79it/s]\n",
            "accuracy: 1.0000, batch_loss: 0.0069, loss: 0.0221 ||: 100%|##########| 13/13 [00:00<00:00, 306.19it/s]\n",
            "accuracy: 0.4737, batch_loss: 1.6745, loss: 1.7688 ||: 100%|##########| 12/12 [00:00<00:00, 702.66it/s]\n",
            "accuracy: 1.0000, batch_loss: 0.0132, loss: 0.0216 ||: 100%|##########| 13/13 [00:00<00:00, 229.26it/s]\n",
            "accuracy: 0.4737, batch_loss: 1.6843, loss: 1.7776 ||: 100%|##########| 12/12 [00:00<00:00, 549.27it/s]\n",
            "accuracy: 1.0000, batch_loss: 0.0063, loss: 0.0206 ||: 100%|##########| 13/13 [00:00<00:00, 277.90it/s]\n",
            "accuracy: 0.4737, batch_loss: 1.6961, loss: 1.7877 ||: 100%|##########| 12/12 [00:00<00:00, 444.59it/s]\n",
            "accuracy: 1.0000, batch_loss: 0.0067, loss: 0.0199 ||: 100%|##########| 13/13 [00:00<00:00, 308.87it/s]\n",
            "accuracy: 0.4737, batch_loss: 1.7073, loss: 1.7975 ||: 100%|##########| 12/12 [00:00<00:00, 613.48it/s]\n",
            "accuracy: 1.0000, batch_loss: 0.0045, loss: 0.0192 ||: 100%|##########| 13/13 [00:00<00:00, 271.13it/s]\n",
            "accuracy: 0.4632, batch_loss: 1.7180, loss: 1.8050 ||: 100%|##########| 12/12 [00:00<00:00, 546.06it/s]\n",
            "accuracy: 1.0000, batch_loss: 0.0598, loss: 0.0207 ||: 100%|##########| 13/13 [00:00<00:00, 241.92it/s]\n",
            "accuracy: 0.4632, batch_loss: 1.7258, loss: 1.8105 ||: 100%|##########| 12/12 [00:00<00:00, 694.89it/s]\n",
            "accuracy: 1.0000, batch_loss: 0.0033, loss: 0.0179 ||: 100%|##########| 13/13 [00:00<00:00, 243.59it/s]\n",
            "accuracy: 0.4632, batch_loss: 1.7377, loss: 1.8202 ||: 100%|##########| 12/12 [00:00<00:00, 518.11it/s]\n",
            "accuracy: 1.0000, batch_loss: 0.0006, loss: 0.0173 ||: 100%|##########| 13/13 [00:00<00:00, 255.29it/s]\n",
            "accuracy: 0.4737, batch_loss: 1.7469, loss: 1.8272 ||: 100%|##########| 12/12 [00:00<00:00, 448.84it/s]\n",
            "accuracy: 1.0000, batch_loss: 0.0301, loss: 0.0178 ||: 100%|##########| 13/13 [00:00<00:00, 335.84it/s]\n",
            "accuracy: 0.4737, batch_loss: 1.7578, loss: 1.8360 ||: 100%|##########| 12/12 [00:00<00:00, 514.08it/s]\n",
            "accuracy: 1.0000, batch_loss: 0.0164, loss: 0.0167 ||: 100%|##########| 13/13 [00:00<00:00, 262.13it/s]\n",
            "accuracy: 0.4737, batch_loss: 1.7666, loss: 1.8439 ||: 100%|##########| 12/12 [00:00<00:00, 396.46it/s]\n",
            "accuracy: 1.0000, batch_loss: 0.1313, loss: 0.0207 ||: 100%|##########| 13/13 [00:00<00:00, 228.88it/s]\n",
            "accuracy: 0.4737, batch_loss: 1.7771, loss: 1.8537 ||: 100%|##########| 12/12 [00:00<00:00, 442.79it/s]\n",
            "accuracy: 1.0000, batch_loss: 0.0198, loss: 0.0160 ||: 100%|##########| 13/13 [00:00<00:00, 266.34it/s]\n",
            "accuracy: 0.4737, batch_loss: 1.7863, loss: 1.8591 ||: 100%|##########| 12/12 [00:00<00:00, 342.83it/s]\n",
            "accuracy: 1.0000, batch_loss: 0.0036, loss: 0.0147 ||: 100%|##########| 13/13 [00:00<00:00, 246.19it/s]\n",
            "accuracy: 0.4737, batch_loss: 1.7981, loss: 1.8692 ||: 100%|##########| 12/12 [00:00<00:00, 488.21it/s]\n",
            "accuracy: 1.0000, batch_loss: 0.0072, loss: 0.0144 ||: 100%|##########| 13/13 [00:00<00:00, 219.30it/s]\n",
            "accuracy: 0.4737, batch_loss: 1.8079, loss: 1.8771 ||: 100%|##########| 12/12 [00:00<00:00, 665.60it/s]\n",
            "accuracy: 1.0000, batch_loss: 0.0013, loss: 0.0138 ||: 100%|##########| 13/13 [00:00<00:00, 266.20it/s]\n",
            "accuracy: 0.4737, batch_loss: 1.8175, loss: 1.8842 ||: 100%|##########| 12/12 [00:00<00:00, 276.97it/s]\n",
            "accuracy: 1.0000, batch_loss: 0.0010, loss: 0.0133 ||: 100%|##########| 13/13 [00:00<00:00, 278.23it/s]\n",
            "accuracy: 0.4737, batch_loss: 1.8269, loss: 1.8917 ||: 100%|##########| 12/12 [00:00<00:00, 388.55it/s]\n",
            "accuracy: 1.0000, batch_loss: 0.0141, loss: 0.0134 ||: 100%|##########| 13/13 [00:00<00:00, 285.15it/s]\n",
            "accuracy: 0.4737, batch_loss: 1.8358, loss: 1.8988 ||: 100%|##########| 12/12 [00:00<00:00, 670.72it/s]\n",
            "accuracy: 1.0000, batch_loss: 0.0184, loss: 0.0133 ||: 100%|##########| 13/13 [00:00<00:00, 259.56it/s]\n",
            "accuracy: 0.4737, batch_loss: 1.8441, loss: 1.9049 ||: 100%|##########| 12/12 [00:00<00:00, 534.12it/s]\n",
            "accuracy: 1.0000, batch_loss: 0.1053, loss: 0.0162 ||: 100%|##########| 13/13 [00:00<00:00, 273.01it/s]\n",
            "accuracy: 0.4737, batch_loss: 1.8541, loss: 1.9146 ||: 100%|##########| 12/12 [00:00<00:00, 363.93it/s]\n",
            "accuracy: 1.0000, batch_loss: 0.0082, loss: 0.0122 ||: 100%|##########| 13/13 [00:00<00:00, 250.22it/s]\n",
            "accuracy: 0.4737, batch_loss: 1.8628, loss: 1.9208 ||: 100%|##########| 12/12 [00:00<00:00, 521.41it/s]\n",
            "accuracy: 1.0000, batch_loss: 0.0018, loss: 0.0115 ||: 100%|##########| 13/13 [00:00<00:00, 301.80it/s]\n",
            "accuracy: 0.4737, batch_loss: 1.8714, loss: 1.9267 ||: 100%|##########| 12/12 [00:00<00:00, 585.89it/s]\n",
            "accuracy: 1.0000, batch_loss: 0.0435, loss: 0.0127 ||: 100%|##########| 13/13 [00:00<00:00, 278.17it/s]\n",
            "accuracy: 0.4737, batch_loss: 1.8812, loss: 1.9351 ||: 100%|##########| 12/12 [00:00<00:00, 540.64it/s]\n",
            "accuracy: 1.0000, batch_loss: 0.0060, loss: 0.0110 ||: 100%|##########| 13/13 [00:00<00:00, 224.30it/s]\n",
            "accuracy: 0.4737, batch_loss: 1.8905, loss: 1.9426 ||: 100%|##########| 12/12 [00:00<00:00, 534.33it/s]\n",
            "accuracy: 1.0000, batch_loss: 0.0031, loss: 0.0106 ||: 100%|##########| 13/13 [00:00<00:00, 257.22it/s]\n",
            "accuracy: 0.4737, batch_loss: 1.8984, loss: 1.9485 ||: 100%|##########| 12/12 [00:00<00:00, 481.34it/s]\n",
            "accuracy: 1.0000, batch_loss: 0.0064, loss: 0.0104 ||: 100%|##########| 13/13 [00:00<00:00, 230.67it/s]\n",
            "accuracy: 0.4737, batch_loss: 1.9070, loss: 1.9552 ||: 100%|##########| 12/12 [00:00<00:00, 968.49it/s]\n",
            "accuracy: 1.0000, batch_loss: 0.0018, loss: 0.0099 ||: 100%|##########| 13/13 [00:00<00:00, 248.57it/s]\n",
            "accuracy: 0.4737, batch_loss: 1.9167, loss: 1.9630 ||: 100%|##########| 12/12 [00:00<00:00, 463.18it/s]\n",
            "accuracy: 1.0000, batch_loss: 0.0020, loss: 0.0097 ||: 100%|##########| 13/13 [00:00<00:00, 193.34it/s]\n",
            "accuracy: 0.4737, batch_loss: 1.9252, loss: 1.9695 ||: 100%|##########| 12/12 [00:00<00:00, 524.45it/s]\n",
            "accuracy: 1.0000, batch_loss: 0.0195, loss: 0.0101 ||: 100%|##########| 13/13 [00:00<00:00, 259.12it/s]\n",
            "accuracy: 0.4737, batch_loss: 1.9332, loss: 1.9764 ||: 100%|##########| 12/12 [00:00<00:00, 315.59it/s]\n",
            "accuracy: 1.0000, batch_loss: 0.0036, loss: 0.0093 ||: 100%|##########| 13/13 [00:00<00:00, 247.47it/s]\n",
            "accuracy: 0.4842, batch_loss: 1.9415, loss: 1.9817 ||: 100%|##########| 12/12 [00:00<00:00, 493.80it/s]\n",
            "accuracy: 1.0000, batch_loss: 0.0041, loss: 0.0090 ||: 100%|##########| 13/13 [00:00<00:00, 253.40it/s]\n",
            "accuracy: 0.4842, batch_loss: 1.9503, loss: 1.9895 ||: 100%|##########| 12/12 [00:00<00:00, 608.55it/s]\n",
            "accuracy: 1.0000, batch_loss: 0.0017, loss: 0.0087 ||: 100%|##########| 13/13 [00:00<00:00, 275.19it/s]\n",
            "accuracy: 0.4842, batch_loss: 1.9588, loss: 1.9971 ||: 100%|##########| 12/12 [00:00<00:00, 615.73it/s]\n",
            "accuracy: 1.0000, batch_loss: 0.0054, loss: 0.0086 ||: 100%|##########| 13/13 [00:00<00:00, 265.09it/s]\n",
            "accuracy: 0.4842, batch_loss: 1.9667, loss: 2.0037 ||: 100%|##########| 12/12 [00:00<00:00, 916.59it/s]\n",
            "accuracy: 1.0000, batch_loss: 0.0084, loss: 0.0085 ||: 100%|##########| 13/13 [00:00<00:00, 252.07it/s]\n",
            "accuracy: 0.4842, batch_loss: 1.9745, loss: 2.0091 ||: 100%|##########| 12/12 [00:00<00:00, 536.01it/s]\n",
            "accuracy: 1.0000, batch_loss: 0.0867, loss: 0.0113 ||: 100%|##########| 13/13 [00:00<00:00, 245.23it/s]\n",
            "accuracy: 0.4842, batch_loss: 1.9817, loss: 2.0154 ||: 100%|##########| 12/12 [00:00<00:00, 620.23it/s]\n",
            "accuracy: 1.0000, batch_loss: 0.0014, loss: 0.0078 ||: 100%|##########| 13/13 [00:00<00:00, 178.99it/s]\n",
            "accuracy: 0.4842, batch_loss: 1.9906, loss: 2.0212 ||: 100%|##########| 12/12 [00:00<00:00, 548.77it/s]\n",
            "accuracy: 1.0000, batch_loss: 0.0141, loss: 0.0081 ||: 100%|##########| 13/13 [00:00<00:00, 256.30it/s]\n",
            "accuracy: 0.4842, batch_loss: 1.9992, loss: 2.0286 ||: 100%|##########| 12/12 [00:00<00:00, 604.97it/s]\n",
            "accuracy: 1.0000, batch_loss: 0.0007, loss: 0.0073 ||: 100%|##########| 13/13 [00:00<00:00, 210.97it/s]\n",
            "accuracy: 0.4842, batch_loss: 2.0060, loss: 2.0338 ||: 100%|##########| 12/12 [00:00<00:00, 494.56it/s]\n",
            "accuracy: 1.0000, batch_loss: 0.0033, loss: 0.0073 ||: 100%|##########| 13/13 [00:00<00:00, 207.24it/s]\n",
            "accuracy: 0.4842, batch_loss: 2.0136, loss: 2.0398 ||: 100%|##########| 12/12 [00:00<00:00, 412.03it/s]\n",
            "accuracy: 1.0000, batch_loss: 0.0013, loss: 0.0070 ||: 100%|##########| 13/13 [00:00<00:00, 233.73it/s]\n",
            "accuracy: 0.4842, batch_loss: 2.0218, loss: 2.0461 ||: 100%|##########| 12/12 [00:00<00:00, 436.38it/s]\n",
            "accuracy: 1.0000, batch_loss: 0.0024, loss: 0.0069 ||: 100%|##########| 13/13 [00:00<00:00, 236.26it/s]\n",
            "accuracy: 0.4842, batch_loss: 2.0281, loss: 2.0512 ||: 100%|##########| 12/12 [00:00<00:00, 552.83it/s]\n",
            "accuracy: 1.0000, batch_loss: 0.0004, loss: 0.0067 ||: 100%|##########| 13/13 [00:00<00:00, 204.40it/s]\n",
            "accuracy: 0.4842, batch_loss: 2.0349, loss: 2.0568 ||: 100%|##########| 12/12 [00:00<00:00, 511.43it/s]\n",
            "accuracy: 1.0000, batch_loss: 0.0025, loss: 0.0066 ||: 100%|##########| 13/13 [00:00<00:00, 276.03it/s]\n",
            "accuracy: 0.4842, batch_loss: 2.0436, loss: 2.0638 ||: 100%|##########| 12/12 [00:00<00:00, 579.99it/s]\n",
            "accuracy: 1.0000, batch_loss: 0.0016, loss: 0.0064 ||: 100%|##########| 13/13 [00:00<00:00, 264.86it/s]\n",
            "accuracy: 0.4842, batch_loss: 2.0506, loss: 2.0695 ||: 100%|##########| 12/12 [00:00<00:00, 558.74it/s]\n",
            "accuracy: 1.0000, batch_loss: 0.0010, loss: 0.0062 ||: 100%|##########| 13/13 [00:00<00:00, 261.26it/s]\n",
            "accuracy: 0.4842, batch_loss: 2.0591, loss: 2.0765 ||: 100%|##########| 12/12 [00:00<00:00, 499.71it/s]\n",
            "accuracy: 1.0000, batch_loss: 0.0013, loss: 0.0061 ||: 100%|##########| 13/13 [00:00<00:00, 205.34it/s]\n",
            "accuracy: 0.4842, batch_loss: 2.0662, loss: 2.0823 ||: 100%|##########| 12/12 [00:00<00:00, 728.20it/s]\n",
            "accuracy: 1.0000, batch_loss: 0.0014, loss: 0.0060 ||: 100%|##########| 13/13 [00:00<00:00, 268.46it/s]\n",
            "accuracy: 0.4842, batch_loss: 2.0738, loss: 2.0885 ||: 100%|##########| 12/12 [00:00<00:00, 553.90it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Finished training...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eM6D_XPMRdiy",
        "outputId": "b1c6f709-c830-423c-b505-185aff58f892"
      },
      "source": [
        "class SentenceClassifierPredictor(Predictor):\n",
        "  def predict(self, sentence: str) -> JsonDict:\n",
        "    return self.predict_json({'sentence': sentence})\n",
        "\n",
        "  def _json_to_instance(self, json_dict: JsonDict) -> Instance:\n",
        "    sentence = json_dict['sentence']\n",
        "    return self._dataset_reader.text_to_instance(sentence)\n",
        "\n",
        "vocab = model.vocab\n",
        "predictor = SentenceClassifierPredictor(model, datasetReader)\n",
        "output = predictor.predict('i want the vaccine')\n",
        "print(\n",
        "  [\n",
        "    (vocab.get_token_from_index(label_id, \"labels\"), prob)\n",
        "      for label_id, prob in enumerate(output[\"probs\"])\n",
        "  ]\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('1', 0.22575588524341583), ('-1', 0.4164092540740967), ('0', 0.3578348457813263)]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:23: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_uZWcMpCijI9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}